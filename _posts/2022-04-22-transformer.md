---
layout: post
title:  "Transformer"
date:   2022-03-30 22:20:01 +0800
categories: [Tech]
tag: 
  - Deep Learning
  - Transformer
---

> å­¦ä¹ ç¬”è®°

# Self Attention

## æ•°å€¼è¿ç®—

![]({{ '/assets/images/posts/2022-04-22-transformer/01.png' | prepend: site.baseurl}})

![]({{ '/assets/images/posts/2022-04-22-transformer/02.png' | prepend: site.baseurl}})

![]({{ '/assets/images/posts/2022-04-22-transformer/03.png' | prepend: site.baseurl}})

![]({{ '/assets/images/posts/2022-04-22-transformer/04.png' | prepend: site.baseurl}})

ç‚¹ç§¯åœ¨æ•°å­¦ä¸­ï¼Œåˆç§°æ•°é‡ç§¯ï¼ˆdot product; scalar product; inner productï¼‰ [é“¾æ¥](https://baike.baidu.com/item/%E7%82%B9%E7%A7%AF/9648528)

![]({{ '/assets/images/posts/2022-04-22-transformer/05.png' | prepend: site.baseurl}})

![]({{ '/assets/images/posts/2022-04-22-transformer/06.png' | prepend: site.baseurl}})

![]({{ '/assets/images/posts/2022-04-22-transformer/07.png' | prepend: site.baseurl}})

## çŸ©é˜µè¿ç®—

![]({{ '/assets/images/posts/2022-04-22-transformer/08.png' | prepend: site.baseurl}})

![]({{ '/assets/images/posts/2022-04-22-transformer/09.png' | prepend: site.baseurl}})

![]({{ '/assets/images/posts/2022-04-22-transformer/10.png' | prepend: site.baseurl}})

![]({{ '/assets/images/posts/2022-04-22-transformer/11.png' | prepend: site.baseurl}})

![]({{ '/assets/images/posts/2022-04-22-transformer/12.png' | prepend: site.baseurl}})

![]({{ '/assets/images/posts/2022-04-22-transformer/13.png' | prepend: site.baseurl}})

![]({{ '/assets/images/posts/2022-04-22-transformer/14.png' | prepend: site.baseurl}})

![]({{ '/assets/images/posts/2022-04-22-transformer/15.png' | prepend: site.baseurl}})

## Positional Encoding

ä¸º self attention æ·»åŠ ä½ç½®ä¿¡æ¯ã€‚æ¯ä¸ªä½ç½®éƒ½æœ‰ä¸€ä¸ªå”¯ä¸€çš„ä½ç½®å‘é‡ ğ‘’ã€‚

![]({{ '/assets/images/posts/2022-04-22-transformer/16.png' | prepend: site.baseurl}})

## seq2seq

![]({{ '/assets/images/posts/2022-04-22-transformer/17.png' | prepend: site.baseurl}})

![]({{ '/assets/images/posts/2022-04-22-transformer/18.png' | prepend: site.baseurl}})

### encoder

![]({{ '/assets/images/posts/2022-04-22-transformer/19.png' | prepend: site.baseurl}})

mutli-head attention çš„è¾“å‡ºå¾€å¾€åŠ ä¸Š fully connected

![]({{ '/assets/images/posts/2022-04-22-transformer/20.png' | prepend: site.baseurl}})

![]({{ '/assets/images/posts/2022-04-22-transformer/21.png' | prepend: site.baseurl}})

### decoder

marked self attention

![]({{ '/assets/images/posts/2022-04-22-transformer/22.png' | prepend: site.baseurl}})

![]({{ '/assets/images/posts/2022-04-22-transformer/23.png' | prepend: site.baseurl}})

![]({{ '/assets/images/posts/2022-04-22-transformer/24.png' | prepend: site.baseurl}})

![]({{ '/assets/images/posts/2022-04-22-transformer/25.png' | prepend: site.baseurl}})

![]({{ '/assets/images/posts/2022-04-22-transformer/26.png' | prepend: site.baseurl}})

## train

![]({{ '/assets/images/posts/2022-04-22-transformer/27.png' | prepend: site.baseurl}})

[äº¤å‰ç†µ (cross entropy)](https://zhuanlan.zhihu.com/p/54066141) 

## ç–‘é—®

Word Embedding

fully connected

feed forward network

residual connection

layer normalization
