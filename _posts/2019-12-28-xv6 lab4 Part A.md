---
layout: post
title:  "6.828 Lab 4: Preemptive Multitasking : Part A"
date:   2019-12-28 21:31:01 +0800
categories: [Tech]
tag: 
  - xv6
  - OS
---

>本文为原创

## Part A: 多处理器支持和多任务协作

在本实验的第一部分，你需要扩展 JOS 以使其能在多处理器系统上运行，然后实现一些新的 JOS 内核系统调用，以允许用户级环境创建其他新环境（环境类似进程）。你还需要实现协作轮转调度(cooperative round-robin scheduling)，在当前环境自愿放弃 CPU（或退出）时，允许内核从一种环境切换到另一种环境。在之后的 C 部分，你将实现抢占式调度，该调度使内核可以在经过一定时间后从环境重新获得对 CPU 的控制，即使环境不合作也是如此。

### 多处理器支持

JOS支持“symmetric multiprocessing”（SMP），SMP 是一种多处理器模型，其中所有 CPU 都具有对系统资源（例如内存和 I / O 总线）的同等访问权限。尽管所有 CPU 在 SMP 中在功能上都是相同的，但是在引导过程中，它们可以分为两种类型：引导处理器（BSP）负责初始化系统和引导操作系统；只有在操作系统启动并运行后，BSP 才会激活应用程序处理器（AP）。BSP 的处理器是由硬件和 BIOS 决定的。到目前为止，你现在所有的JOS代码都已在 BSP 上运行。

在SMP系统中，每个 CPU 都有一个 local APIC（LAPIC）单元。LAPIC 单元负责在整个系统中传递中断。LAPIC 还为其连接的CPU提供唯一的标识符。在本实验中，我们利用 LAPIC 单元的以下基本功能（在 kern/lapic.c 中）：

* 读取 LAPIC 标识符（APIC ID）以了解我们的代码当前在哪个 CPU 上运行（请参阅 cpunum()）。

* 从 BSP 发送启动处理器间中断（IPI）到 AP 以启动其他 CPU（请参阅 lapic_startap()）。

* 在 C 部分中，我们对 LAPIC 的内置计时器进行编程，通过触发时钟中断来支持抢先式多任务处理（请参阅apic_init()）。

处理器使用 memory-mapped I/O (MMIO) 访问其 LAPIC。在 MMIO 中，一部分物理内存会直接存入某些 I/O 设备的寄存器，因此通常用于访问内存的相同加载/存储指令可使用设备寄存器访问。你已经在物理地址 0xA0000 上看到了一个 I/O 区域（我们使用它来写入 VGA 显示缓冲区），输出缓冲区是 0xf00b8000。LAPIC 位于一个从地址 0xFE000000（比 4GB 少 32MB）开始的区域中，因此对于我们来说，直接映射到 KERNBASE 的空间很大。在 JOS 虚拟内存中映射了一个 4MB 空间的 `MMIOBASE`，所以我们有地方映射这样的设备。由于以后的实验会介绍更多的MMIO 区域，因此您需要编写一个简单的函数来从该区域分配空间并将设备内存映射到该区域。

#### Exercise 1

在 `kern/pmap.c` 中实现 `mmio_map_region`。要了解其用法，请查看 `kern/lapic.c` 中 `lapic_init` 的开头。在测试 mmio_map_region 之前，你必须先做下一个练习。

mmio_map_region 的代码如下：

```c
    size = ROUNDUP(size, PGSIZE);
    if((base + size > MMIOLIM) || (base + size < base))
        panic("Overflow in mmio region");
    boot_map_region(kern_pgdir, base, size, pa, PTE_PCD | PTE_PWT | PTE_W);
    base += size; // base是全局变量，同时它将保存mmio中的可用空间的首地址
    return (void*)(base - size);
```

#### Application Processor Bootstrap

在启动 AP 之前，BSP 应该首先收集有关多处理器系统的信息，例如 CPU 的总数，它们的 APIC ID 和 LAPIC 单元的 MMIO 地址。`kern/mpconfig.c` 中的 `mp_init()` 函数通过读取驻留在 BIOS 内存区域中的 MP 配置表来检索此信息。

`boot_aps()` 函数(在 `kern/init.c` 中)驱动 AP 引导进程。AP 在实模式下启动，就像引导加载程序在 `boot/boot.S` 中启动的方式一样，因此 `boot_aps()` 将 AP 入口代码 (`kern/mpentry.S`) 复制到可在实模式下寻址的内存位置。与引导加载程序不同，我们可以控制AP从何处开始执行代码。我们将代码复制到 `0x7000`(`MPENTRY_PADDR`)，尽管在低位640KB的内存的任何未使用的，与页面对齐的物理地址都可以使用。

之后，`boot_aps()` 通过向相应 AP 的 LAPIC 单元发送 `STARTUP IPI` 以及一个初始 `CS:IP` 地址来依次激活AP，并且 AP 应该从 `MPENTRY_PADDR` 处开始运行。`kern/mpentry.S` 中的入口代码与 `boot/boot.S` 的入口代码非常相似。进行一些简单的设置后，它将使 AP 进入启用分页的保护模式，然后调用C设置例程 `mp_main()` (也在 `kern/init.c` 中)。`boot_aps()`等待 AP 在其 `struct CpuInfo` 的 cpu_status 字段中发 `CPU_STARTED` 信号，然后再继续唤醒下一个。

##### Exercise 2

阅读 `kern/init.c` 中的 `boot_aps()` 和 `mp_main()`，以及 `kern/mpentry.S` 中的汇编代码。确保您了解 AP 引导程序的控制流转换过程。然后在 `kern/pmap.c` 中修改您对 `page_init()` 的实现，以避免将 `MPENTRY_PADDR` 所在的页面被添加到空闲列表中，以便我们可以安全地在该物理地址复制并运行 AP 引导程序代码。你的代码应该能够通过被更新的 `check_page_free_list()` 的测试（但可能无法通过 `check_kern_pgdir()` 的测试）。

page_init 应该修改为下面：

```c
void
page_init(void)
{
    size_t i;
    for (i = 0; i < npages; i++) {
        if(i == 0) {
            pages[i].pp_ref = 1;
            pages[i].pp_link = NULL;
        } else if(i * PGSIZE >= IOPHYSMEM && i * PGSIZE <= PADDR(boot_alloc(0))) {
            pages[i].pp_ref = 1;
            pages[i].pp_link = NULL;
        } else if(i * PGSIZE == MPENTRY_PADDR) {  
            // 这里是标记 MPENTRY_PADDR 所在的物理页
            pages[i].pp_ref = 1;
            pages[i].pp_link = NULL;
        } else {
            pages[i].pp_ref = 0;
            pages[i].pp_link = page_free_list;
            page_free_list = &pages[i];
        }
    }
}
```

使用 `make qemu-nox` 出现如下，无法通过 `check_kern_pgdir()` 的测试：

```c
check_page_free_list() succeeded!
check_page_alloc() succeeded!
check_page() succeeded!
kernel panic on CPU 0 at kern/pmap.c:852: assertion failed: check_va2pa(pgdir, base + KSTKGAP + i) == PADDR(percpu_kstacks[n]) + i
```

##### Question 1

对比 `kern/mpentry.S` 与 `boot/boot.S`。请记住，`kern/mpentry.S` 就像内核中的所有其他内容一样经过编译和链接，在 `KERNBASE` 之上运行，宏 `MPBOOTPHYS` 的作用是什么？为什么在 `kern/mpentry.S` 中有必要，但在 `boot/boot.S` 中却没有？换句话说，如果在 `kern/mpentry.S` 中省略了它，那会出什么问题？

提示：回忆一下我们在实验1中讨论的链接地址和加载地址之间的区别。

通过响应来自启动 CPU 的 STARTUP IPI 信号，来启动每个非启动 CPU (`AP`)。 AP 将在实模式下启动，并将 `CS:IP`设置为 `XY00:0000`，其中 XY 是随 STARTUP 发送的 8 位值。 因此，此代码必须从 4096 字节边界开始。

因为此代码将 DS 设置为零，所以它必须从低 2 ^ 16 字节物理内存中的地址开始运行。

boot_aps() (在 init.c 中)将此代码复制到 `MPENTRY_PADDR` (满足上述限制)。 然后，对于每个 AP，它将预先分配的每个内核堆栈的地址存储在 `mpentry_kstack` 中，发送 `STARTUP IPI`，并等待此代码确认它已启动(在 init.c 中的 `mp_main` 中发生)。

此代码类似于 boot/boot.S，除了

* 不需要启用 A20
* 它使用 `MPBOOTPHYS` 来计算其绝对地址
* 依赖于符号，而不是依靠链接器填充它们

AP 是从实模式下开始运行，所以需要通过 MPBOOTPHYS 宏的转换虚拟地址，运行这部分代码。boot.S 中不需要这个转换是因为代码的本来就被加载在实模式可以寻址的地方。

#### Per-CPU State and Initialization

编写多处理器 OS 时，区分每个处理器专用的 CPU 状态和整个系统共享的全局状态是非常重要的。`kern/cpu.h` 定义了CPU的大多数状态，包括存储每个 CPU 变量的 `struct CpuInfo` 结构。`cpunum()`始终返回调用它的 CPU 的 ID，该 ID 可用作 `cpus` 这样的数组的索引。 另外，宏 `thiscpu` 是当前 CPU 的 `struct CpuInfo` 的简写。

这是你应注意的 CPU 状态：

* Per-CPU kernel stack  
    由于多个 CPU 可以进入内核中，因此我们需要为每个处理器使用单独的内核堆栈，以防止它们干扰彼此的执行。数组`percpu_kstacks[NCPU][KSTKSIZE]`为 NCPU 的内核堆栈保留了空间。  

    在实验 2 中，你映射了 BSP 内核堆栈的物理内存 `bootstack`，在 `KSTACKTOP` 的下方。 同样，在本实验中，您将把每个CPU 的内核堆栈映射到该区域，其中保护页充当它们之间的缓冲区。 CPU 0的堆栈仍将从 `KSTACKTOP` 增长； CPU 1 的堆栈将从 CPU 0 的堆栈底部开始的 `KSTKGAP` 字节后开始，依此类推。 `inc/memlayout.h` 显示了映射布局。

* Per-CPU TSS and TSS descriptor  
    为了指定每个 CPU 的内核堆栈所在的位置，还需要每个 CPU 的任务状态段 (TSS)。 CPU i 的 TSS 存储在 `cpus[i].cpu_ts` 中，并且相应的 TSS 描述符在 GDT 条目 `gdt[(GD_TSS0 >> 3)+ i]` 中定义。 在 `kern/trap.c`中定义的全局 ts 变量将不再有用。

* Per-CPU current environment pointer  
    由于每个 CPU 可以同时运行不同的用户进程，因此我们将符号 `curenv` 重新定义为引用 `cpus[cpunum()].cpu_env`(或 `thiscpu->cpu_env`)，它指向在当前 CPU 上执行的环境(代码正在运行)。

* Per-CPU system registers  
    所有寄存器，包括系统寄存器，都是 CPU 专用的。 因此，初始化这些寄存器的指令，例如 lcr3()，ltr()，lgdt()，lidt() 等，必须在每个 CPU 上执行一次。 为此，定义了函数 `env_init_percpu()` 和 `trap_init_percpu()`。

    除此之外，如果你在解决方案中添加了任何额外的 CPU 状态或执行了其他任何特定于 CPU 的初始化 (例如，在 CPU 寄存器中设置新位) 以挑战早期实验中的问题，请确保复制它们到每个 CPU 上！

##### Exercise 3

修改 `mem_init_mp()` (在 `kern/pmap.c` 中)以映射从 `KSTACKTOP` 开始的每个 CPU 堆栈，如 `inc/memlayout.h` 中所示。 每个堆栈的大小为 `KSTKSIZE` 字节加上未映射的保护页的 `KSTKGAP` 字节。 你的代码应该能通过 `check_kern_pgdir()` 中的检查。

mem_init_mp 代码如下：

```c
static void mem_init_mp(void)
{
    for(int i = 0 ; i < NCPU ; i ++) {
        uintptr_t va = KSTACKTOP - i * (KSTKSIZE + KSTKGAP);
        boot_map_region(kern_pgdir, va - KSTKSIZE, KSTKSIZE,
            PADDR(percpu_kstacks[i]), PTE_W | PTE_P);
    }
}
```

如果在进行分支合并的时候，出现了一定问题，比如没有将调用 mem_init_map 函数的代码合并进来，可以在 mem_init 函数中添加。

```c
check_kern_pgdir() succeeded!
check_page_free_list() succeeded!
check_page_installed_pgdir() succeeded!
SMP: CPU 0 found 1 CPU(s)
enabled interrupts: 1 2
[00000000] new env 00001000
kernel panic on CPU 0 at kern/trap.c:328: Page Fault in Kernel-Mode at 00000000.
```

##### Exercise 4

`trap_init_percpu()` (`kern/trap.c`)中的代码初始化 BSP 的 TSS 和 TSS 描述符。它能在实验 3 中工作，但在其他 CPU 上运行时不正确。更改代码，使其可以在所有 CPU 上使用。(注意：新代码不应再使用全局 ts 变量。)

代码如下：

```c
void trap_init_percpu(void)
{
    // Setup a TSS so that we get the right stack when we trap to the kernel.
    thiscpu->cpu_ts.ts_esp0 = (uintptr_t)(percpu_kstacks[cpunum()] + KSTKSIZE);
    thiscpu->cpu_ts.ts_ss0 = GD_KD;
    thiscpu->cpu_ts.ts_iomb = sizeof(struct Taskstate);

    // Initialize the TSS slot of the gdt.
    gdt[(GD_TSS0 >> 3) + cpunum()] = SEG16(STS_T32A, (uint32_t) (&(thiscpu->cpu_ts)),
                    sizeof(struct Taskstate) - 1, 0);
    gdt[(GD_TSS0 >> 3) + cpunum()].sd_s = 0;

    // 加载 TSS 选择器（与其他分段选择器一样，底部的三位是特殊的；我们将其保留为0）
    ltr(GD_TSS0 + (cpunum() << 3));

    // Load the IDT
    lidt(&idt_pd);
}
```

使用 `make qemu-nox CPUS=4`，可以得到如下：

```c
check_page_free_list() succeeded!
check_page_alloc() succeeded!
check_page() succeeded!
check_kern_pgdir() succeeded!
check_page_free_list() succeeded!
check_page_installed_pgdir() succeeded!
SMP: CPU 0 found 4 CPU(s)
enabled interrupts: 1 2
SMP: CPU 1 starting
SMP: CPU 2 starting
SMP: CPU 3 starting
[00000000] new env 00001000
kernel panic on CPU 0 at kern/trap.c:326: Page Fault in Kernel-Mode at 00000000.
```

内核错误，在完成 Exercise6 后，就不会出现了

#### Locking

在 `mp_main()` 中初始化 AP 之后，我们当前的代码会自旋。在让 AP 进一步执行之前，我们需要首先解决多个 CPU 同时运行内核代码时的竞争状态。实现它的最简单方法的是使用大内核锁。大内核锁是单个的全局锁，每当环境进入内核模式时都会被持有，并在环境返回到用户模式时释放。在此模型中，用户模式下的环境可以在任何可用的 CPU 上同时运行，但是内核模式下只能运行一个环境。任何其他尝试进入内核模式的环境都必须等待。

`kern/spinlock.h`声明了大的内核锁，即`kernel_lock`。它还提供`lock_kernel()`和`unlock_kernel()`，这是获取和释放锁的快捷方式。你应该在四个位置应用大内核锁：

* 在 `i386_init()` 中，在 BSP 唤醒其他 CPU 之前获取锁。
* 在 `mp_main()` 中，在初始化 AP 之后获取锁，然后调用 `sched_yield()` 开始在此 AP 上运行环境。
* 在 `trap()` 中，从用户模式发生异常时获取锁。要确定异常是在用户模式下还是内核模式下发生的，请检查 `tf_cs` 的低位。
* 在 `env_run()` 中，在切换到用户模式之前立即释放锁。不要太早或太晚地这样做，否则会遇到竞争或死锁。

[锁 参考](https://hehao98.github.io/posts/2019/04/xv6-3/)

##### Exercise 5

如上所述，通过在适当的位置调用 `lock_kernel()` 和 `unlock_kernel()` 来应用大内核锁。

`kern/init.c` 下的 `i386_init`：

```c
    // Acquire the big kernel lock before waking up APs
    // Your code here:
    lock_kernel();
```

`kern/init.c` 下的 `mp_main`:

```c
    // Now that we have finished some basic setup, call sched_yield()
    // to start running processes on this CPU.  But make sure that
    // only one CPU can enter the scheduler at a time!
    //
    // Your code here:
    lock_kernel();
    sched_yield();
```

`kern/trap.c` 下的 `trap`:

```c
    if ((tf->tf_cs & 3) == 3) {
        // Trapped from user mode.
        // Acquire the big kernel lock before doing any
        // serious kernel work.
        // LAB 4: Your code here.
        lock_kernel();
        assert(curenv);
```

`kern/env.c` 下的 `env_run`:

```c
void env_run(struct Env *e)
{
    if (curenv != NULL && curenv->env_status == ENV_RUNNING)
        curenv->env_status = ENV_RUNNABLE;
    curenv = e;
    curenv->env_status = ENV_RUNNING;
    curenv->env_runs++;
    lcr3(PADDR(curenv->env_pgdir));
    unlock_kernel();
    env_pop_tf(&(curenv->env_tf));
}
```

##### Question 2

似乎使用大内核锁可以保证一次只有一个 CPU 可以运行内核代码。 为什么每个 CPU 仍需要单独的内核堆栈？ 描述一个即使使用大内核锁保护，使用共享内核堆栈也会出错的情况。

答: CPU0 正在处理用户态的中断，内核栈中存储了栈帧，此时 CPU1 的用户态程序也发生中断。因为锁是在 trap 函数中被调用的，在发生中断时，会先压栈再调用 trap 函数。

### 轮循调度

本实验中的下一个任务是更改 JOS 内核，以便它可以“循环”方式在多个环境之间交替。 JOS 中的循环调度工作方式如下：

* `kern/sched.c` 中的函数 `sched_yield()` 负责选择要运行的新环境。 它以循环方式依次搜索 `envs []` 数组，从先前运行的环境之后开始（如果没有先前运行的环境，则从数组的开头开始），选择状态为 `ENV_RUNNABLE` 的第一个环境（请参见 `inc/env.h`），然后调用 `env_run()` 运行该环境。
* `sched_yield()`绝对不能在两个 CPU 上同时运行相同的环境。它可以表明某个环境当前正在某些 CPU（可能是当前 CPU ）上运行，因为该环境的状态为 `ENV_RUNNING`。
* 我们为你实现了一个新的系统调用 `sys_yield()`，用户环境可以调用这个系统调用来调用内核的 `sched_yield()` 函数，从而自动将 CPU 让给其他的环境。

#### Exercise 6

如上所述，在 `sched_yield()` 中实现循环调度。不要忘记修改 `syscall()` 来调用 `sys_yield()`。

确保在 `mp_main` 中调用 `sched_yield()`。

修改 `kern/init.c` 以创建三个（或更多！）环境，这些环境都运行程序 `user/yield.c`。

sched_yield 的实现：

```c
void sched_yield(void)
{
    struct Env *idle;

    // Implement simple round-robin scheduling.
    //
    // 在此环境最后一次运行 env 之后，以循环方式在 'envs' 中搜索 ENV_RUNNABLE 环境。
    // 切换到找到的第一个这样的环境。
    //
    // 如果没有可运行的环境，但是以前在此 CPU 上运行的环境仍然是 ENV_RUNNING，则可以选择该环境。
    //
    // 永远不要选择当前正在另一个 CPU 上运行的环境 (env_status == ENV_RUNNING)。
    // 如果没有可运行的环境，只需跳至下面的代码即可停止 CPU。
    // LAB 4: Your code here.
    idle = curenv;
    int idle_envid = (idle == NULL) ? -1 : ENVX(idle->env_id);
    int i;

    // 查找 idle 之后的环境
    for (i = idle_envid + 1; i < NENV; i++) {
        if (envs[i].env_status == ENV_RUNNABLE) {
            env_run(&envs[i]);
        }
    }

    // 查找 idle 之前的环境
    for (i = 0; i < idle_envid; i++) {;
        if (envs[i].env_status == ENV_RUNNABLE) {
            env_run(&envs[i]);
        }
    }

    if(idle != NULL && idle->env_status == ENV_RUNNING) {
        env_run(idle);
    }

    // sched_halt never returns
    sched_halt();
}
```

syscall 函数添加如下代码：

```c
case SYS_yield:
    sys_yield();
    ret = 0;
    break;
```

不要忘记移除在 `kern/init.c mp_main()` 中的如下代码

```c
// Remove this after you finish Exercise 6
for (;;);
```

`kern/init.c` 中的 `i386_init` 增加并修改如下代码：

```c
#if defined(TEST)
    // Don't touch -- used by grading script!
    ENV_CREATE(TEST, ENV_TYPE_USER);
#else
    // Touch all you want.
    // ENV_CREATE(user_primes, ENV_TYPE_USER);
    ENV_CREATE(user_yield, ENV_TYPE_USER);
    ENV_CREATE(user_yield, ENV_TYPE_USER);
    ENV_CREATE(user_yield, ENV_TYPE_USER);
#endif // TEST*
```

使用 `make qemu-nox CPUS=2` 或者 `make qemu CPUS=2` 得到：

```c
Hello, I am environment 00001000.
Hello, I am environment 00001001.
Back in environment 00001000, iteration 0.
Hello, I am environment 00001002.
Back in environment 00001001, iteration 0.
Back in environment 00001000, iteration 1.
Back in environment 00001002, iteration 0.
Back in environment 00001001, iteration 1.
Back in environment 00001000, iteration 2.
Back in environment 00001002, iteration 1.
Back in environment 00001001, iteration 2.
Back in environment 00001000, iteration 3.
Back in environment 00001002, iteration 2.
Back in environment 00001001, iteration 3.
Back in environment 00001000, iteration 4.
Back in environment 00001002, iteration 3.
All done in environment 00001000.
[00001000] exiting gracefully
[00001000] free env 00001000
Back in environment 00001001, iteration 4.
Back in environment 00001002, iteration 4.
```

在 yield 程序退出之后，系统中将没有可运行的环境，调度程序应调用 JOS 内核监视器。如果以上任何一种都没有发生，请在继续操作之前先修改你的代码。

#### Question 3

在 `env_run()` 的实现中，你应该调用了 `lcr3()`。 在调用 `lcr3()` 之前和之后，你的代码（至少应该）引用变量`e`（即 `env_run` 的参数）。 加载 `％cr3` 寄存器后，MMU 使用的寻址上下文将立即更改。 但是虚拟地址（即 `e`）相对于给定的地址上下文具有含义-地址上下文指定了虚拟地址映射到的物理地址。为什么在寻址切换之前和之后都可以取消对指针 `e` 的引用？

因为 e 指向的在 envs 中，而在创建环境的时候，envs 会被复制到环境中。

#### Question 4

每当内核从一种环境切换到另一种环境时，都必须确保保存了旧环境的寄存器，以便以后可以正确还原它们。为什么？这在哪里发生？

在 trap 处理函数中，将栈帧地址赋值给 env_tf 域。

### System Calls for Environment Creation

尽管你的内核现在可以在多个用户级别的环境中运行和切换，但仍限于内核最初设置的运行环境。现在，你将实现必要的JOS系统调用，以允许用户环境创建和启动其他新的用户环境。

Unix提供 `fork()` 系统调用作为其进程创建原语。 Unix `fork()` 复制调用进程（父进程）的整个地址空间，以创建一个新进程（子进程）。从用户空间可观察到的两个唯一区别是它们的进程 ID 和父进程 ID（由 `getpid` 和 `getppid` 返回）。在父进程中，`fork()` 返回子进程的进程 ID，而在子进程中，`fork()` 返回 0。默认情况下，每个进程都获得自己的私有地址空间，并且另一个进程对内存的修改对其他进程都不可见。

你将提供一组不同的，更原始的 JOS 系统调用，以创建新的用户模式环境。通过这些系统调用，除了其他样式的环境创建之外，你还可以完全在用户空间中实现类似 Unix 的 `fork()`。你将为JOS编写的新系统调用如下：

sys_exofork:  
该系统调用创建了一个几乎空白的新环境：在其地址空间的用户部分中未映射任何内容，并且该环境不可运行。调用`sys_exofork` 时，新环境将与父环境具有相同的寄存器状态。在父级中，sys_exofork 将返回新创建的环境的 envid_t（如果环境分配失败，则返回负数的错误代码）。但是，在子级中，它将返回 0。（由于该子级开始时标记为不可运行，因此 sys_exofork 实际上不会返回该子级，直到父级通过使用...标记该子级可显式允许该操作为止。）

sys_env_set_status:  
将指定环境的状态设置为 `ENV_RUNNABLE` 或 `ENV_NOT_RUNNABLE`。一旦其地址空间和寄存器状态已完全初始化，此系统调用通常用于标记准备运行的新环境。

sys_page_alloc:  
分配一页物理内存，并将其映射到指定环境的地址空间中的给定虚拟地址。

sys_page_map:  
将页面映射（不是页面的内容！）从一个环境的地址空间复制到另一个环境，保留被分配的内存共享，以便新映射和旧映射都引用同一物理内存页面。

sys_page_unmap:  
取消映射在给定环境中映射到给定虚拟地址的页面。

对于上面所有接受环境 ID 的系统调用，JOS 内核支持以下约定：值 0 表示 `当前环境`。此约定由 `kern/env.c` 中的`envid2env()` 实现。

我们在测试程序 `user/dumbfork.c` 中提供了一个类似于Unix的 `fork()` 的非常原始的实现。该测试程序使用上述系统调用来创建和运行带有其自身地址空间副本的子环境。然后，像前面的练习一样，使用 `sys_yield` 在两个环境之间来回切换。父级在 10 次迭代后退出，而子级在 20 次迭代后退出。

#### Exercise 7

在 `kern/syscall.c` 中实现上述系统调用，并确保 `syscall()` 调用它们。您将需要在 `kern/pmap.c` 和 `kern/env.c`中使用各种功能，尤其是 `envid2env()`。现在，无论何时调用 `envid2env()`，都要在 `checkperm` 参数中传递 1。确保检查所有无效的系统调用参数，在这种情况下返回 `-E_INVAL`。使用 `user/dumbfork` 测试您的 JOS 内核，并在继续之前确保其工作正常。

sys_exofork 代码如下：

```c
static envid_t sys_exofork(void)
{
    // 从 kern / env.c 使用 env_alloc() 创建新环境。
    // 应该将其保留为 env_alloc 创建的状态，除了将状态设置为ENV_NOT_RUNNABLE,
    // 并从当前环境复制寄存器集外，但进行了调整，因此sys_exofork看起来将返回0。

    // LAB 4: Your code here.
    struct Env* e;
    int res = env_alloc(&e, curenv->env_id);
    if(res >= 0) {
        e->env_status = ENV_NOT_RUNNABLE;
        e->env_tf = curenv->env_tf;
        e->env_tf.tf_regs.reg_eax = 0;
        return e->env_id;
    }
    return res;
}
```

sys_env_set_status 函数如下：

```c
static int sys_env_set_status(envid_t envid, int status)
{
    // LAB 4: Your code here.
    if (status == ENV_RUNNABLE || status == ENV_NOT_RUNNABLE)
    {
        struct Env* e;
        int res = envid2env(envid, &e, 1);
        if(res == 0)
            e->env_status = status;
        return res;
    } else
        return -E_INVAL;

}
```

sys_page_alloc的代码如下：

```c
static int sys_page_alloc(envid_t envid, void *va, int perm)
{

    // LAB 4: Your code here.
    // panic("sys_page_alloc not implemented");
    if ((uintptr_t) va >= UTOP || PGOFF(va) != 0 || (~perm & (PTE_U | PTE_P))) {
        return -E_INVAL;
    }

    if ((perm & ~(PTE_SYSCALL)) != 0) {
        return -E_INVAL;
    }

    struct Env* e;
    if (envid2env(envid, &e, 1) < 0) {
        return -E_BAD_ENV;
    }

    struct PageInfo* p = page_alloc(1);
    if (p == NULL)
        return -E_NO_MEM;

    int res = page_insert(e->env_pgdir, p, va, perm);
    if (res < 0)
        page_free(p);
    return res;
}
```

sys_page_map代码如下：

```c
// perm 具有与 sys_page_alloc 中相同的限制，除了不能授予对只读页面的写访问权限。
static int
sys_page_map(envid_t srcenvid, void *srcva, envid_t dstenvid, void *dstva, int perm)
{

    // LAB 4: Your code here.
    if ((uintptr_t)srcva >= UTOP || PGOFF(srcva) != 0)
        return -E_INVAL;
    if ((uintptr_t)dstva >= UTOP || PGOFF(dstva) != 0)
        return -E_INVAL;
    if ((perm & PTE_U) == 0 || (perm & PTE_P) == 0 || (perm & ~PTE_SYSCALL) != 0)
        return -E_INVAL;
    struct Env *src_e, *dst_e;

    // 需不需要设置 checkperm，未知
    if (envid2env(srcenvid, &src_e, 1) < 0 || envid2env(dstenvid, &dst_e, 0) < 0)
        return -E_BAD_ENV;

    pte_t *pte;
    struct PageInfo *pp = page_lookup(src_e->env_pgdir, srcva, &pte);

    if ((*pte & PTE_W) == 0 && (perm & PTE_W) == PTE_W)
        return -E_INVAL;

    if (page_insert(dst_e->env_pgdir, pp, dstva, perm) < 0)
        return -E_NO_MEM;
    return 0;
}
```

这里有一个坑，envid2env 的 checkperm 的参数，必须是 0，如果设置了这个参数，那么 envid 必须指向当前进程或者当前进程的子进程，否则就会返回一个错误的结果。

sys_page_unmap 代码如下：

```c
static int
sys_page_unmap(envid_t envid, void *va)
{
    // LAB 4: Your code here.
    if ((uintptr_t) va >= UTOP || PGOFF(va) != 0) {
        return -E_INVAL;
    }
    struct Env *e;
    if (envid2env(envid, &e, 1) < 0) {
        return -E_BAD_ENV;
    }
    page_remove(e->env_pgdir, va);
    return 0;
}
```

syscall 函数添加如下代码：

```c
case SYS_exofork:
    ret = sys_exofork();
    break;
case SYS_env_set_status:
    ret = sys_env_set_status(a1, a2);
    break;
case SYS_page_alloc:
    ret = sys_page_alloc(a1, (void *) a2, a3);
    break;
case SYS_page_map:
    ret = sys_page_map(a1, (void *) a2, a3, (void *) a4, a5);
    break;
case SYS_page_unmap:
    ret = sys_page_unmap(a1, (void *) a2);
    break;
```

`make grade`，现在能通过 Part A 的测试了
