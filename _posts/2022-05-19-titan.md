---
layout: post
title:  "TITAN"
date:   2022-05-19 22:10:01 +0800
categories: [Paper]
tag: 
  - Deep Learning
  - 生物信息
---

> 笔记

## 研究动因

找到一种能够独立研究未知 TCR 和抗原靶点 (表位) 的泛化能力的方法。

## 结果

通过在原子水平上用 SMILES 序列编码表位，利用迁移学习和数据扩充来丰富输入数据空间并提高性能。TITAN 在预测未知 TCR 的特异性方面取得了很高的性能 (10 倍 CV 中的 ROC-AUC 为 0.87)，并大大超过了当前最新技术 (ImRex) 的结果。值得注意的是，基于 Levenshtein 距离的 K-NN 分类器在未知的 TCR 上也表现出了有竞争力的性能。虽然对未知表位的推广仍然具有挑战性，但是有两个重大突破。首先，通过分析注意热图，证明了可用表位数据的稀疏性有利于将表位作为类进行隐式处理。对于足够复杂的模型，这可能是限制未知表位性能的一个普遍问题。第二，TITAN 在未知的表位上表现出显著改善的性能，并且能够将注意力集中在具有化学意义的分子结构上。 

## 简介

TITAN (TCR 表位双峰注意网络) 利用双峰神经网络结构显式编码 TCR 和表位序列。更具体地说，TITAN 使用卷积来聚合局部信息并融合模式，使用可解释的注意机制来预测绑定概率。

使用 SMILES 有效地将 TCR-epitope 结合问题重新表述为更一般的复合蛋白质相互作用（CPI）任务，从而能够使用蛋白质-配体结合亲和力的大型数据库进行预训练，例如 BindingDB，包括超过100万个标记样本

## 方法

### 数据

将 VDJ 数据库中收集的数据 (Bagaev 等人，2019 年) 与 ImmuneRACE 项目在 2019 冠状病毒疾病特定数据集 (Dines 等人，2020 年) 相结合。由于成对链数据仍然很少，仅限于使用 TCR β 链序列。

由于 VDJ 数据集高度不平衡，所以排除了相关 TCR 序列少于 15 个的表位，并将样本减少到每个表位 400 个 TCR 的限度。经过这些预处理步骤后，得到了一个包含 10599 个 TCR 序列和 87 个表位样本的数据集。

对于新冠病毒数据集，为了避免歧义，只保留与单个独特表位相关的样本，排除非生产性序列。然后，应用与 VDJ 数据集相同的预处理步骤，将样本减少到 400 个TCR/表位，并排除具有少于 15 个相关 TCR 的表位，获得 12996 个示例的数据集。

为避免不平衡的数据集，通过重组 TCR 和表位的配对，可以匹配每个 TCR 的阴性示例数和阳性示例数。通过这个过程，构建了一个 46290 个样本的训练数据集，其中 50% 是阳性的，包含 192 个不同的表位。

### 模型

TCR 序列由 BLOSUM62 矩阵编码。抗原肽使用 SMILES 编码，有助于数据扩充。

![]({{ '/assets/images/posts/2022-05-19-titan/01.png' | prepend: site.baseurl}})

在输入序列上使用三个具有核尺寸为 3、 5 和 11 的卷积的并行通道来组合来自不同空间扩展的局部邻域的信息。第四信道具有没有卷积的剩余连接。对于四个通道中的每一个，都使用两个 attention layer，其中一个 attention layer 用 context 来计算相对于另一个 attention layer 的 attention score。这允许模型使用来自binding partner (即 context) 的信息来了解输入序列中每个标记 (即 reference) 的重要性。

TCR 表位结合预测的基线模型由 k-nearest-neighbor (k-NN) 分类器构成。作为样本之间的距离度量，利用各个表位和 TCR 蛋白质一级序列的长度归一化 Levenshtein 距离之和。

### 验证

在 10 倍交叉验证分割上评估模型。为了分别确定模型对未知 TCR 和未知表位的泛化能力，使用了两种不同的拆分方法。

第一种，确保验证数据集不包含训练数据集中的 TCR，而表位不做处理，将此拆分称为 TCR 拆分。

第二种，确保每个在训练集中 TCR 和表位都不出现在验证集中，称为严格的拆分。

关于预训练，测试了两种不同的设置，一种是在微调过程中可以调整所有权重，另一种是半冷冻设置，只允许 TCR 通道和表位的 dense layer 的权重变化

![]({{ '/assets/images/posts/2022-05-19-titan/02.png' | prepend: site.baseurl}})

K-NN 是指基线模型。其他缩写表示泰坦在不同的环境下接受训练。

AA CDR3: 表位的氨基酸编码，TCR 仅输入 CDR3 序列；

AA full: 表位氨基酸编码，TCR 全序列输入；

SMI CDR3: 表位的 SMILES 编码，TCR 仅输入 CDR3 序列；

SMI-full: 表位的 SMILES 编码，TCR 的全序列输入；

pretrained: 表位的 SMILES 编码，TCR 的全序列输入，BindingDB 上的模型预训练；

pretrained semifrozen: 表位的 SMILES 编码，TCR 的全序列输入，BindingDB 上的模型预训练，微调期间表位通道中的权重固定；

pretrained aug：表位的 SMILES 编码，TCR 的全序列输入，在 BindingDB 上进行模型预训练，并进行数据扩充；

pretrained semifrozen aug：表位的 SMILES 编码，TCR 的全序列输入，在 BindingDB 上进行模型预训练并增加数据，在微调期间固定表位通道中的权重。

TCR 拆分: 

![]({{ '/assets/images/posts/2022-05-19-titan/03.png' | prepend: site.baseurl}})

strict 拆分:

![]({{ '/assets/images/posts/2022-05-19-titan/04.png' | prepend: site.baseurl}})

虽然在 TCR 分割交叉验证中，ROC-AUC 在训练期间不断增加，但它在严格分割的训练期间是停滞的

![]({{ '/assets/images/posts/2022-05-19-titan/05.png' | prepend: site.baseurl}})

实验构建的模型和 imrex 在未知表位的泛化能力:

![]({{ '/assets/images/posts/2022-05-19-titan/06.png' | prepend: site.baseurl}})
