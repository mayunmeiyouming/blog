---
layout: post
title:  "pMTnet"
date:   2022-06-08 10:10:01 +0800
categories: [Paper]
tag: 
  - Deep Learning
  - 生物信息
---

> 笔记

采用了分阶段的方法，将学习抗原 TCR 结合特异性 (pMHCs) 的目标分为三个步骤，以降低预测任务的难度。首先，使用 LSTM 网络训练 pMHCs 的 numeric embedding  (仅 I 类)，以便抗原和 MHC 的蛋白质序列可以数字表示。其次，使用堆叠式自动编码器训练 TCR 序列的 embedding，该编码器再次对 TCR 序列的文本字符串进行数字编码。这两个步骤创建可用于数学运算的数字向量，并为最终配对预测奠定基础。在最后阶段，我们在这两个 embedding 的基础上创建了一个深层神经网络，以生物学意义的方式将来自 TCR、抗原肽序列和 MHC 等位基因的信息结合起来。采用微调来最终确定 TCR 和 pMHCs 之间配对的预测模型。 

为了在数值上嵌入 TCR。我们首先使用 Atchley 因子对氨基酸符号进行编码，该因子使用五个数字来综合表示每个氨基酸的物理化学性质。然后，我们构建了一个堆叠式自动编码器（图1a），将来自 243747 个人类TCR β CDR3 序列中 Atchley 版本的 TCR 作为输入，学习 TCR 的数值嵌入。自动编码器能够通过无监督的分解-重建过程捕获复杂输入的关键特征，并将捕获的输入特征嵌入到短数字向量中。虽然只使用了 CDR3 β 序列，但这些由 V、D 和 J 基因组成的 CDR3 ，已经将它们的身份间接的注入到 embedding 中。通过比较输入 TCR 和重构 TCR，我们验证了该自动编码器。我们的分析表明，可以通过 CDR 3 embedding 以高度可靠的方式重建 CDR 3（图 1 b），这证明了这种自动编码器的能够被成功训练。更多示例如扩展数据图 1 a、b 所示。原始 TCR CDR3 Atchley 矩阵和`重构矩阵`之间的 Pearson 相关性通常大于 0.95 (扩展数据图 1 c)。

对于 pMHCs 的 embedding，首先使用深层 LSTM 神经网络 (图 1c) 重新实现 netMHCpan 模型，以便 netMHCpan 模型的内部层可与我们模型的其他部分集成。该模型的输入是 MHC 序列（仅 I 类）和抗原蛋白序列。在这一阶段的训练中，输出是抗原是否与 MHC 分子结合。虽然该输出层专门用于预测抗原和 MHC 结合，但它之前的层应包含有关 pMHC 复合物整体结构的重要信息。使用用于训练 netMHCpan 的相同数据对我们的模型进行再训练；这些数据包括 172422 项肽-MHC 结合亲和力测量，涵盖 130 种人类 I 类 MHC。在独立的测试数据集中，预测的结合概率和真实结合强度的 Pearson 相关性达到 0.781 (图 1d)，这与原始 netMHCpan 中的 Pearson 相关性 0.76 相当。为了下一阶段学习 TCR 和 pMHCs 之间的配对，我们提取了最终输出层之前的直接层（数字向量）作为 pMHCs 的 numeric embedding。

最后，利用训练好的 TCR 和 pMHCS 的数字矢量编码来学习它们之间的配对。我们基于这两个子模型的输出构建了一个完全连接的深度学习网络，最终形成一个带有单个神经元的层，用于预测配对（图 1e）。在此集成模型的基础上，我们创新性地采用了一种差分学习模式，在该模式中，在每个训练周期中，向模型输入一个真实的 TCR 和 pMHC 绑定对，以及另一个具有相同 pMHC 的负对。从一系列同行的出版物和四个铬单细胞免疫分析溶液数据集 （N=19219) 中收集了总共 32607 对 TCR–pMHC 结合。一些数据库提供了质量指标，我们使用这些指标来过滤记录，以保持高置信度的数据对。例如，在 VDJdb 数据中，我们只包含带有 vdj 的记录。已删除重复记录。通过随机错配这 32607 对的 TCR 和 pMHCs，创建了十倍多的负对。进行了 150 轮的训练（图 1f）。将 pMHC-TCR 结合预测网络的最终模型命名为 pMTnet。在差异训练之后，还以比较的方式生成了预测输出。pMTnet 输出一个介于 0 和 1 之间的连续变量，反映了 TCR 和 pMHC 之间预测结合强度的百分位。重要的是，由于总是将抗原和 MHC 捆绑在一起，并让模型专注于区分是否结合 TCR，所有验证都是专门用于区分 TCR 结合特异性，而不是抗原-MHC 结合或整体免疫原性。 

在独立实验数据中，pMTnet 预测 TCR–pMHC 配对。我们收集的大量已知 TCR–pMHC 结合对，并进行了一系列验证分析。首先收集了 619 对经实验验证的 TCR–pMHC 结合对。在本次和以下所有验证分析中，删除了训练数据集中出现的 TCR–pMHC 对，因此测试集完全独立于训练集。随机配对产生的负对数量增加了 10 倍。使用了两个指标，ROC 和精确召回率 (PR) 的曲线面积 (AUC) 。引人注目的是，在该测试队列中，ROC 的 AUC 达到 0.827，PR 的 AUC 达到 0.566（图 2a）。为了测试 pMTnet 是否真正了解了决定绑定的特征，或者只是记住了配对情况，研究了与训练 TCR 具有不同相似度的 TCR 的预测性能（图 2b，左组）。为了计算相似度，我们基于 TCR embedding 计算了每个测试 TCR 到所有训练 TCR 的最小欧氏距离。显示了测试 TCR 子集的 ROC 和 PR 的 AUC。我们对 pMHC 进行了相同的分析并进行了类似的观察（图 2b，右图组）。

我们发现 T 细胞克隆型的克隆大小和预测等级与取得的统计显著性呈负相关（图 2c）。换言之，预测中 pMHC 结合强度更强的 TCR 的 T 细胞比没有强结合伙伴的 T 细胞扩张得更多。相反，我们观察到一些克隆大小较小的 TCR 与 pMHC 的预测结合等级较小，这可能是由于 TCR 与 pMHC 之间结合的随机性，也可能是由于不断引入的新克隆尚未发生扩展所致。

pMTnet 对每种肽类似物（与 MHC 复合物）进行了预测，并且预测强结合类似物确实比其类似物具有更强的结合强度

我们发现，与非接触残基相比，接触残基更有可能导致预测的 pMHC 结合强度发生更大的变化（图 4b；P 值 = 0.036）。我们还进行了硅丙氨酸扫描，发现了类似的趋势（第三或第四段与任何其他段之间的 t 检验 P值 < 0.0001，图 4c）。丙氨酸扫描不如零位扫描显著，这可能是因为在丙氨酸扫描中，所有丙氨酸都被认为在突变后没有作用（丙氨酸 → 丙氨酸）；然而，用具有大侧链的其他残基替换一个丙氨酸可能会影响蛋白质复合物的整体结构完整性，这实际上可能导致结合亲和力的变化。在图 4a-c中，我们显示了等级百分位数的绝对变化（变化为更强或较弱的结合）；然而，对百分位变化方向的检查表明，硅内突变主要导致较弱的结合。 

HERV-E 抗原比新抗原和其他自身抗原更有可能具有免疫原性，这证实了过去关于 HERV-E 在肾癌诱导免疫反应中重要性的报告 

NIES 比其他候选生物标志物具有更强的预测能力

NIES 越高的患者总体生存率越高

## 嵌入 TCR CDR3 β 序列

我们通过 Atchley 因子编码 TCR CDR3 β序列，该因子用五个数值表示每个氨基酸。这五个值可以综合表征每种氨基酸的生化特性。生成的数值矩阵的行数为 Atchley 因子数，列数为 80。然后将 TCR 序列的 Atchley 矩阵输入堆叠式自动编码器，这是一种能够以无监督方式学习复杂信号的强大算法。TCR 序列的 Atchley 矩阵被输入到一个包含 30 个 5×2 核的二维卷积层中，并用 SELU 函数激活，然后是一个批量归一化层和一个包含 4×1 核的二维平均池层。池层之后是另一个具有 20 个 4×2 内核的二维卷积层，以及同一批规范化层和一个二维平均池层，如前所述。合并后，矩阵被转换为一个扁平层，然后是一个用SELU函数激活的30个神经元密集层，一个用 SELU 函数激活的 30 个神经元密集层，一个用 0.01 的退出率激活的 30 个神经元密集层，另一个用 SELU 函数激活的 30 个神经元密集层，这是自动编码器模型的瓶颈层。瓶颈层之前的层被反转以创建模型的解码器部分。编码器的输入和解码器的输出与 Atchley 矩阵完全相同。训练过程指示自动编码器重建输入数据，并使用简单的数字向量捕获其固有结构。训练完成后，自动编码器中间最小的完全连接层（瓶颈）形成原始 CDR3 的 30 个神经元数字向量嵌入。由于多种原因，TCR CDR3 β 序列被填充到 80 个氨基酸长。我们为将来可能添加 α 链的 CDR3 留下了空间。还可以添加 CDR1 和 CDR2。这可能很方便，因为即使添加了其他 CDR，自动编码器的结构也不需要更改，或者只需要最小程度的更改 

## 嵌入 pMHCs

pMHCs的嵌入主要遵循netMHCpan算法。netMHCpan算法使用伪序列方法对 MHC 蛋白进行编码。伪序列由与肽接触的氨基酸组成，仅包括 34 个多态性残基。然后使用 BLOSUM50 矩阵对这 34 个残基进行编码。另一方面，（neo）抗原也由 BLOSUM50 基质编码，如 netMHCpa n中所述。我们构建了一个以 HLA 伪序列和抗原序列为输入的深度学习模型。在这里，我们使用 MHC 序列而不是类型作为输入，以便将使用范围扩展到训练数据之外的 MHC 类型。我们的实现与原始 netMHCpan 模型的主要区别在于，我们使用了输出大小为 16 的 LSTM 层（位于抗原输入顶部）和输出大小为 16 的 LSTM 层（位于 MHC 输入顶部），而不是简单的前馈神经网络。我们发现这一变化似乎提高了我们达到模型收敛的速度。抗原和 MHC 的 LSTM 输出串联在一起，在同一层中形成一个 32 维向量。该层之后是一个致密层，其中 60 个神经元被 tan h激活，一个神经元致密层作为最后一个输出层。我们使用用于训练 netMHCpan 模型的精确数据来训练该网络。训练完成后，我们直接提取在单个神经元输出层（又是一个短的数字向量）之前的 60 维完全连接层的输出，作为 pMHCs 的 embedding。


我们采用转移学习来利用训练过的 TCR 和 pMHC 数字编码。这些预训练模型被固定并作为早期层纳入最终预测模型（保存训练所需的参数）。这两种编码都以数字向量的形式生成最终输出层。我们将两个数值向量连接成一个单层，添加一个由 300 个神经元被 RELU 激活的致密层，一个脱落率为 0.2 的脱落层，一个由 200 个神经元被 RELU 激活的致密层，一个由 100 个神经元被 RELU 激活的致密层，以及最后一个由单个神经元被 tanh 激活的致密层


